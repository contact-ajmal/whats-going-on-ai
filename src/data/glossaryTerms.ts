export const glossaryTerms: Record<string, string> = {
    "LLM": "Large Language Model. A type of AI model trained on massive amounts of text data to understand and generate human-like language (e.g., GPT-4, Claude 3).",
    "Transformer": "A deep learning architecture introduced in 2017 ('Attention Is All You Need') that weighs the influence of different parts of input data. It is the foundation of modern LLMs.",
    "Neural Network": "A computational model inspired by the human brain, consisting of interconnected nodes (neurons) that process information in layers.",
    "Generative AI": "AI systems capable of generating new content, such as text, images, or code, in response to prompts.",
    "RAG": "Retrieval-Augmented Generation. A technique that enhances LLM responses by retrieving relevant data from an external knowledge base before generating an answer.",
    "Fine-tuning": "The process of taking a pre-trained model and training it further on a specific dataset to specialize it for a particular task.",
    "Hallucination": "When an AI model generates confident but incorrect or nonsensical information.",
    "Inference": "The process of using a trained AI model to make predictions or generate outputs based on new input data.",
    "Parameters": "The internal variables (weights and biases) learned by the model during training. More parameters generally correlate with higher capability.",
    "Zero-shot Learning": "The ability of a model to perform a task it wasn't explicitly trained for, based on its general understanding.",
    "Multimodal": "AI models that can misunderstand and process multiple types of data simultaneously, such as text, images, and audio (e.g., GPT-4V, Gemini).",
    "Agent": "An AI system capable of perceived autonomy, using tools and planning steps to achieve a goal.",
    "Token": "The basic unit of text used by LLMs. A token can be a word, part of a word, or a character (approx 0.75 words)."
};
